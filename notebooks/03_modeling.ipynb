{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Modeling & Interpretation\n",
    "This notebook handles model training, hyperparameter tuning, artifact generation, and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Ensure project root is in path for src imports\n",
    "sys.path.append('..')\n",
    "from src.features import add_engineered_features\n",
    "from src.evaluation import generate_shap_plots\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "os.makedirs('../reports', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing & Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processing columns\n",
    "numeric_features = ['income', 'loan_amount', 'loan_duration_months', 'credit_score', 'age', 'previous_defaults', 'debt_to_income', 'monthly_payment']\n",
    "categorical_features = ['employment_type']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "df = pd.read_csv('../data/raw/example.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "X_eng = add_engineered_features(X)\n",
    "X_proc = preprocessor.fit_transform(X_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "Using `RandomizedSearchCV` to optimize XGBoost and Random Forest for ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 3.1 XGBoost Tuning\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=42),\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_search.fit(X_proc, y)\n",
    "print(f\"Best XGB ROC-AUC: {xgb_search.best_score_:.4f}\")\n",
    "\n",
    "# 3.2 Random Forest Tuning\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='roc_auc',\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_search.fit(X_proc, y)\n",
    "print(f\"Best RF ROC-AUC: {rf_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finalize Best Model\n",
    "Compare and save the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = xgb_search.best_estimator_ if xgb_search.best_score_ >= rf_search.best_score_ else rf_search.best_estimator_\n",
    "\n",
    "joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "joblib.dump(best_model, '../models/final_model.pkl')\n",
    "\n",
    "print(f\"Final model ({type(best_model).__name__}) and preprocessor saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Unit Tests & Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Triggering pytest for model validation...\")\n",
    "!python -m pytest ../tests/test_model.py\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "importances = best_model.feature_importances_\n",
    "feat_importances = pd.Series(importances, index=feature_names)\n",
    "top_5 = feat_importances.nlargest(5).to_dict()\n",
    "joblib.dump(top_5, '../models/top_features.pkl')\n",
    "\n",
    "generate_shap_plots(best_model, X_proc, feature_names, output_path='../reports/shap_summary.png')\n",
    "print(\"Artifacts updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}